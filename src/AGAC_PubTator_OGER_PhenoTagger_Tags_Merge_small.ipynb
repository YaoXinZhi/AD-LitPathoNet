{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4d8e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59356505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "改代码用于将 AGAC, PubTator, OGER, Phenotagger 的标注进行合并\n",
    "\n",
    "并且生成任务二的输入文件\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8f0ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGAC and PubTator\n",
    "tagging_file = '../data/ad-total-20220414/ad-RE_20220410/ad.total.tagging.txt'\n",
    "\n",
    "# OGER\n",
    "oger_file = '../data/ad-total-20220414/ad-tagger_20220415/ad.total.oger.tsv'\n",
    "\n",
    "# PhenoTagger\n",
    "phenotagger_file = '../data/ad-total-20220414/ad-tagger_20220415/ad.total.phenotagger.txt'\n",
    "\n",
    "# 下次合并的时候需要以原始的sent文件为基底来生成merge tagging文件\n",
    "# 和生成infer_input文件\n",
    "\n",
    "\n",
    "# merged tagging save file\n",
    "tagging_save_file = '../data/ad-total-20220414/ad-RE_20220410/ad.total.merge.tagging.txt'\n",
    "\n",
    "# re input save file\n",
    "re_input_save_file = '../data/ad-total-20220414/ad-RE_20220410/ad.re.input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ed51c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read HPO file\n",
    "def read_hpo_tagger_file(hpo_tag_file: str):\n",
    "\n",
    "    print('Reading HPO tagger file.')\n",
    "    pmid_to_hpo = defaultdict(set)\n",
    "    with open(hpo_tag_file) as f:\n",
    "        for line in f:\n",
    "            l = line.strip().split('\\t')\n",
    "            if len(l) < 5:\n",
    "                continue\n",
    "\n",
    "            pmid = l[0]\n",
    "\n",
    "            tag = l[3]\n",
    "            tag_type = l[4]\n",
    "            tag_id = l[5]\n",
    "            # confidence = l[6]\n",
    "\n",
    "            pmid_to_hpo[pmid].add((tag, tag_type, tag_id))\n",
    "\n",
    "    return pmid_to_hpo\n",
    "\n",
    "# read OGER file\n",
    "def read_oger_tagger_file(oger_tag_file: str):\n",
    "    \n",
    "    pmid_sent_id_to_tag = {}\n",
    "    \n",
    "    with open(oger_tag_file) as f:\n",
    "        for line in f:\n",
    "            l = line.strip().split('\\t')\n",
    "            \n",
    "            sent_id = l[0]\n",
    "            pmid = l[1]\n",
    "            sent = l[2]\n",
    "            tag = eval(l[3])\n",
    "            \n",
    "            pmid_sent_id_to_tag[(pmid, sent_id)] = tag\n",
    "    return pmid_sent_id_to_tag\n",
    "\n",
    "# 将AGAC，PubTator，OGER 和 PhenoTagger的标签合并成一个tagging文件\n",
    "def merge_tag(tagging_file: str, pmid_to_hpo: dict, pmid_sent_id_to_tag: dict, save_file: str):\n",
    "    \n",
    "    # 零时数据观察\n",
    "    go_type_set = set()\n",
    "    cellular_component_set = set()\n",
    "    \n",
    "    wf = open(save_file, 'w')\n",
    "    with open(tagging_file) as f:\n",
    "        for line in f:\n",
    "            l = line.strip().split('\\t')\n",
    "            \n",
    "            # PMC:0001\n",
    "            sent_idx = l[0]\n",
    "            pmid = l[1]\n",
    "            sentence = l[2]\n",
    "            \n",
    "            tag_set = set()\n",
    "            for tag in l[3:]:\n",
    "                # ('TMEM106B', 'Gene', (50, 58))\n",
    "                tag_set.add(eval(tag))\n",
    "                \n",
    "                \n",
    "            # add HPO tag\n",
    "            if pmid_to_hpo.get(pmid):\n",
    "                #print(pmid_to_hpo[pmid])\n",
    "                #input()\n",
    "                for hpo_tag in pmid_to_hpo[pmid]:\n",
    "                    tag = hpo_tag[0]\n",
    "                    if tag in sentence:\n",
    "                        mention, hpo_type, hpo_id = hpo_tag\n",
    "                        \n",
    "                        offset_set = find_offset(mention, sentence)\n",
    "                        \n",
    "                        for offset in offset_set:\n",
    "                            tag_set.add((*hpo_tag, offset))\n",
    "                        \n",
    "            \n",
    "            # add GO tag\n",
    "            # 0001\n",
    "            sent_id = sent_idx.split(':')[1]\n",
    "            if pmid_sent_id_to_tag.get((pmid, sent_id)):\n",
    "                \n",
    "                #print(pmid_sent_id_to_tag[(pmid, sent_id)])\n",
    "                #input()\n",
    "                \n",
    "                for go_tag in pmid_sent_id_to_tag[(pmid, sent_id)]:\n",
    "                    mention = go_tag[0]\n",
    "                    go_type = go_tag[2]\n",
    "                    go_id = go_tag[3]\n",
    "                    offset = go_tag[4]\n",
    "                    \n",
    "                    # 零时数据观察\n",
    "                    go_type_set.add(go_type)\n",
    "                    if go_type == 'cellular_component':\n",
    "                        cellular_component_set.add(go_tag[1])\n",
    "                    \n",
    "                    #print(go_type)\n",
    "                    \n",
    "                    tag_set.add((mention, go_type, go_id, offset))\n",
    "                \n",
    "                \n",
    "                #tag_set.update(pmid_sent_id_to_tag[(pmid, sent_id)])\n",
    "        \n",
    "            tag_wf = '\\t'.join(map(str, tag_set))\n",
    "            wf.write(f'{sent_idx}\\t{pmid}\\t{sentence}\\t{tag_wf}\\n')\n",
    "    wf.close()\n",
    "    print(f'{save_file} save done.')\n",
    "    \n",
    "    print(go_type_set)\n",
    "\n",
    "    \n",
    "def find_offset(token: str, sent: str):\n",
    "    \n",
    "    offset_set = set()\n",
    "    \n",
    "    token_end = 0\n",
    "    while True:\n",
    "        token_start = sent.find(token, token_end)\n",
    "        token_end = token_start + len(token)\n",
    "        \n",
    "        if token_start == -1:\n",
    "            break\n",
    "            \n",
    "        offset_set.add((token_start, token_end))\n",
    "\n",
    "    return offset_set\n",
    "\n",
    "\n",
    "# tagging merge\n",
    "def tagging_merge(pubtator_set: set, model_tag_set: set):\n",
    "\n",
    "    # Disease, Interaction, Enzyme, NegReg, Reg, Protein, MPA, CPA, Var,\n",
    "    # todo: 20220414 把OGER和PhenoTagger的标签也加进去\n",
    "    pubtator_label_to_agac = {\n",
    "        # Pubtator\n",
    "        'Disease': 'Disease',\n",
    "        'DNAMutation': 'Var',\n",
    "        'Gene': 'Gene',\n",
    "        'ProteinMutation': 'Var',\n",
    "        'SNP': 'Var',\n",
    "        # SETH\n",
    "        'SUBSTITUTION': 'Var',\n",
    "        'DBSNP_MENTION': 'Var',\n",
    "        'DELETION': 'Var',\n",
    "        'FRAMESHIFT': 'Var',\n",
    "        'INSERTION': 'Var',\n",
    "        # PhenoTagger\n",
    "        'Phenotype': 'Disease',\n",
    "        # OGER\n",
    "        # 这一类别的GO不太好归进去\n",
    "        #'cellular_component': '',\n",
    "        'molecular_function': 'MPA',\n",
    "        'biological_process': 'CPA',\n",
    "    }\n",
    "\n",
    "    special_save = {'p.M239V', 'AD'}\n",
    "    \n",
    "    pubtator_tag_set = {(tag[0], pubtator_label_to_agac[tag[1]], tag[-1]) for tag in pubtator_set\n",
    "                        if pubtator_label_to_agac.get(tag[1])}\n",
    "\n",
    "    pubtator_token2label = {token: label for token, label, _ in pubtator_tag_set}\n",
    "\n",
    "    # 2021 05 25\n",
    "    # if a token appears in both pubtator and model labels,\n",
    "    # but has a different label,\n",
    "    # keep pubtator label.\n",
    "    # Avoid nested entities\n",
    "    merge_tag_set = pubtator_tag_set.copy()\n",
    "\n",
    "    for m_token, m_label, m_offset in model_tag_set:\n",
    "        \n",
    "        (m_start, m_end) = m_offset\n",
    "        \n",
    "        m_start = int(m_start)\n",
    "        m_end = int(m_end)\n",
    "        \n",
    "        save_flag = True\n",
    "        for p_token, p_label, p_offset in pubtator_tag_set:\n",
    "            (p_start, p_end) = p_offset\n",
    "            \n",
    "            p_start = int(p_start)\n",
    "            p_end = int(p_end)\n",
    "            # m_token in p_token\n",
    "            if m_start >= p_start and m_end <= p_end:\n",
    "                save_flag = False\n",
    "                continue\n",
    "            # p_token in m_token\n",
    "            if p_start >= m_start and p_end <= m_end:\n",
    "                # do not replace 527\n",
    "                # if (p_token, p_label, p_offset) in merge_tag_set:\n",
    "                #     merge_tag_set.remove((p_token, p_label, p_offset))\n",
    "                # merge_tag_set.add((m_token, m_label, m_offset))\n",
    "                save_flag = False\n",
    "                continue\n",
    "\n",
    "            # m_token cross p_token and m_token is in front of p_token\n",
    "            if m_start <= p_start and (p_start <= m_end <= m_end):\n",
    "                save_flag = False\n",
    "                continue\n",
    "\n",
    "            # p_token cross m_token and p_token is in front of m_token\n",
    "            if p_start <= m_start and (m_start <= p_end <= m_end):\n",
    "                save_flag = False\n",
    "                pass\n",
    "\n",
    "        if save_flag:\n",
    "            merge_tag_set.add((m_token, m_label, m_offset))\n",
    "\n",
    "    # merge label to pubtator label\n",
    "    merge_tag_set_cp = merge_tag_set.copy()\n",
    "    for token, label, offset in merge_tag_set_cp:\n",
    "        if pubtator_token2label.get(token):\n",
    "            if label == pubtator_token2label[token]:\n",
    "                continue\n",
    "            else:\n",
    "                merge_tag_set.remove((token, label, offset))\n",
    "                merge_tag_set.add((token, pubtator_token2label[token], offset))\n",
    "\n",
    "    merge_tag_set_cp = merge_tag_set.copy()\n",
    "\n",
    "    # tagging filter\n",
    "    for info in merge_tag_set_cp:\n",
    "        (token, label, (start, end)) = info\n",
    "        if token in special_save:\n",
    "            continue\n",
    "        if len(token) <= 2:\n",
    "            if info in merge_tag_set:\n",
    "                merge_tag_set.remove(info)\n",
    "                continue\n",
    "\n",
    "        if '(' == token[0] \\\n",
    "                and ')' == token[-1] \\\n",
    "                and label in ['Protein', 'Gene', 'Var']:\n",
    "            merge_tag_set.remove(info)\n",
    "            merge_tag_set.add((token[1:-1], label, (start+1, end-1)))\n",
    "            continue\n",
    "\n",
    "        if '(' == token[0] \\\n",
    "                and label in ['Protein', 'Gene', 'Var'] \\\n",
    "                and len(token) >= 3:\n",
    "            merge_tag_set.remove(info)\n",
    "            merge_tag_set.add((token[1:], label, (start+1, end)))\n",
    "            continue\n",
    "\n",
    "        if ')' == token[-1] and label in ['Protein', 'Gene', 'Var'] and len(token) >= 3:\n",
    "            merge_tag_set.remove(info)\n",
    "            merge_tag_set.add((token[:-1], label, (start, end-1)))\n",
    "            continue\n",
    "\n",
    "        if ';' == token[-1] or ':' == token[-1]:\n",
    "            merge_tag_set.remove(info)\n",
    "            token = token[:-1]\n",
    "            end -= 1\n",
    "            info = (token, label, (start, end))\n",
    "            merge_tag_set.add(info)\n",
    "            continue\n",
    "\n",
    "        if \"'s\" == token[-2:]:\n",
    "            merge_tag_set.remove(info)\n",
    "            token = token[:-2]\n",
    "            end -= 2\n",
    "            info = (token, label, (start, end))\n",
    "            merge_tag_set.add(info)\n",
    "            continue\n",
    "\n",
    "        if '(' in token \\\n",
    "                or ')' in token \\\n",
    "                or ',' in token \\\n",
    "                or ' and ' in token \\\n",
    "                or len(token)> 50:\n",
    "            if '(' in token \\\n",
    "                    and ')' in token \\\n",
    "                    and label in ['Protein', 'Gene', 'Var']\\\n",
    "                    and 'and' not in token\\\n",
    "                    and 3 <=len(token) <= 30:\n",
    "                continue\n",
    "            merge_tag_set.remove(info)\n",
    "            continue\n",
    "\n",
    "    merge_tag_set_cp = merge_tag_set.copy()\n",
    "    # tagging merge\n",
    "    for info_1 in merge_tag_set_cp:\n",
    "        for info_2 in merge_tag_set_cp:\n",
    "            (token_1, label_1, (start_1, end_1)) = info_1\n",
    "            (token_2, label_2, (start_2, end_2)) = info_2\n",
    "            \n",
    "            \n",
    "            start_1 = int(start_1)\n",
    "            end_1 = int(end_1)\n",
    "            start_2 = int(start_2)\n",
    "            end_2 = int(end_2)\n",
    "            \n",
    "            if token_1 in special_save:\n",
    "                continue\n",
    "\n",
    "            if len(token_1.split()) == 1 and label_1 in ['Protein', 'Gene', 'Var', 'Disease']:\n",
    "                continue\n",
    "\n",
    "            if (start_1>=start_2 and end_1<=end_2) \\\n",
    "                    and info_1 != info_2:\n",
    "\n",
    "                if start_1 == start_2 and end_1 == end_2:\n",
    "                    if label_1 == 'Protein' and label_2 == 'Gene':\n",
    "                        if info_1 in merge_tag_set:\n",
    "                            merge_tag_set.remove(info_1)\n",
    "                    if label_2 == 'Protein' and label_1 == 'Gene':\n",
    "                        if info_2 in merge_tag_set:\n",
    "                            merge_tag_set.remove(info_2)\n",
    "                else:\n",
    "                    if info_1 in merge_tag_set:\n",
    "                        #print(info_1)\n",
    "                        merge_tag_set.remove(info_1)\n",
    "                        #merge_tag_set.remove((token_1, label_1, (start_1, end_1)))\n",
    "    return merge_tag_set\n",
    "\n",
    "\n",
    "    \n",
    "# 用tagging文件生成 任务二的输入文件\n",
    "# 生成的规则和原来 NER infer_result_process.py 里用的一样\n",
    "def generate_re_input_file_from_tagging(tagging_file: str, save_file: str):\n",
    "    \n",
    "    \n",
    "    non_self_label = {'Protein', 'Gene', \"Enzyme\", 'Var', 'Disease'}\n",
    "    \n",
    "    reg_set = {'Reg', 'PosReg', 'NegReg'}\n",
    "    \n",
    "    save_count = 0\n",
    "    wf = open(save_file, 'w')\n",
    "    with open(tagging_file) as f:\n",
    "        for line in f:\n",
    "            l = line.strip().split('\\t')\n",
    "            sent_id = l[0]\n",
    "            pmid = l[1]\n",
    "            sentence = l[2]\n",
    "            tag_set = [eval(tag) for tag in l[3:]]\n",
    "            \n",
    "            agac_tag_set = set()\n",
    "            other_tag_set = set()\n",
    "            \n",
    "            # 分AGAC标签和标准化了的（PubTator，OGER，PhenoTagger）的标签\n",
    "            for tag in tag_set:\n",
    "                if len(tag) == 3:\n",
    "                    agac_tag_set.add(tag)\n",
    "                else:\n",
    "                    other_tag_set.add(tag)\n",
    "                    \n",
    "            merged_tagging_set = tagging_merge(other_tag_set, agac_tag_set)\n",
    "            \n",
    "            for (tag1, tag2) in permutations(merged_tagging_set, 2):\n",
    "\n",
    "                token1, label1, offset1 = tag1\n",
    "                token2, label2, offset2 = tag2\n",
    "\n",
    "                if label1 in non_self_label and label2 in non_self_label and label1 == label2:\n",
    "                    continue\n",
    "\n",
    "                if label1 == 'Var' and label1 == label2:\n",
    "                    continue\n",
    "\n",
    "                save_count += 1\n",
    "\n",
    "                if label1 in reg_set and label2 not in reg_set:\n",
    "                        # print(label1, label2)\n",
    "                        # input('continue')\n",
    "                    continue\n",
    "                wf.write(f'{token1}\\t{label1}\\t{offset1}\\t'\n",
    "                            f'{token2}\\t{label2}\\t{offset2}\\t'\n",
    "                            f'None\\t{sentence}\\t{pmid}\\t{sent_id}\\n')\n",
    "    \n",
    "    wf.close()\n",
    "    print(f'{save_file} save done.')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e74e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading HPO tagger file.\n"
     ]
    }
   ],
   "source": [
    "pmid_to_hpo = read_hpo_tagger_file(phenotagger_file)\n",
    "\n",
    "pmid_sent_id_to_tag = read_oger_tagger_file(oger_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ab9df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ad-total-20220414/ad.total.merge.tagging.txt save done.\n",
      "{'cellular_component', 'molecular_function', 'biological_process'}\n"
     ]
    }
   ],
   "source": [
    "merge_tag(tagging_file, pmid_to_hpo, pmid_sent_id_to_tag, tagging_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "23a6f470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ad-total-20220414/ad.re.input.txt save done.\n"
     ]
    }
   ],
   "source": [
    "generate_re_input_file_from_tagging(tagging_save_file, re_input_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef828949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26c8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f018d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761d349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfb4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
